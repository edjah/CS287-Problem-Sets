{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kMy8khN8tvG"
   },
   "source": [
    "# HW 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HAY0edmR8tvI"
   },
   "source": [
    "In this homework you will build a full neural machine translation system using an attention-based encoder-decoder network to translate from German to English. The encoder-decoder network with attention forms the backbone of many current text generation systems. See [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) for an excellent tutorial that also contains many modern advances.\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a non-attentional baseline model (pure seq2seq as in [ref](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)). \n",
    "2. Incorporate attention into the baseline model ([ref](https://arxiv.org/abs/1409.0473) but with dot-product attention as in class notes).\n",
    "3. Implement beam search: review/tutorial [here](http://www.phontron.com/slides/nlp-programming-en-13-search.pdf)\n",
    "4. Visualize the attention distribution for a few examples. \n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n",
    "\n",
    "This will be the most time-consuming assignment in terms of difficulty/training time, so we recommend that you get started early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2PbuMxo8tvI"
   },
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. Feel free to construct your models inline, or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "0FNVUhWw85ZW",
    "outputId": "58285e3b-a8e4-4d8e-b037-6881d0c45046"
   },
   "outputs": [],
   "source": [
    "# !pip install -q torch torchtext spacy opt_einsum\n",
    "# !pip install -qU git+https://github.com/harvardnlp/namedtensor\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ea-ruBUP8tvJ"
   },
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# Text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data, datasets\n",
    "\n",
    "# Named Tensor wrappers\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "\n",
    "# Word vectors\n",
    "from torchtext.vocab import GloVe, FastText\n",
    "\n",
    "# utilities for logging time\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AGtH_W2-8tvM"
   },
   "source": [
    "We first need to process the raw data using a tokenizer. We are going to be using spacy, but you can use your own tokenization rules if you prefer (e.g. a simple `split()` in addition to some rules to acccount for punctuation), but we recommend sticking to the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPkWuOiV8tvN"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "teP-tJBp8tvP"
   },
   "source": [
    "Note that we need to add the beginning-of-sentence token `<s>` and the end-of-sentence token `</s>` to the \n",
    "target so we know when to begin/end translating. We do not need to do this on the source side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eq7IABHi8tvQ"
   },
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "\n",
    "DE = NamedField(names=('srcSeqlen',), tokenize=tokenize_de)\n",
    "\n",
    "# only target needs BOS/EOS\n",
    "EN = NamedField(\n",
    "    names=('trgSeqlen',), tokenize=tokenize_en,\n",
    "    init_token = BOS_WORD, eos_token = EOS_WORD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsFvHSZm8tvS"
   },
   "source": [
    "Let's download the data. This may take a few minutes.\n",
    "\n",
    "While this dataset of 200K sentence pairs is relatively small compared to others, it will still take some time to train. We only expect you to work with sentences of length at most 20 for this homework. You are expected to train on at least this reduced dataset for this homework, but are free to experiment with the rest of the training set as well.\n",
    "\n",
    "**We encourage you to start with `MAX_LEN=20` but encourage experimentation after getting reasonable results with the filtered data. The baseline scores are based on models train on the filtered data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "j-s1HsaA8tvT",
    "outputId": "7ae62297-059e-41c2-f322-69ea0aecdb03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': <namedtensor.text.torch_text.NamedField object at 0x7fa558b797b8>, 'trg': <namedtensor.text.torch_text.NamedField object at 0x7fa558b79710>}\n",
      "119076\n",
      "{'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'], 'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.']}\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 20\n",
    "def filter_pred(x):\n",
    "    return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
    "    \n",
    "train, val, test = datasets.IWSLT.splits(\n",
    "    exts=('.de', '.en'), fields=(DE, EN), filter_pred=filter_pred,\n",
    ")\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "papOb0pJ8tvY",
    "outputId": "284e365e-9bae-4663-8a7a-8f7a3207f0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Gallo: Das ist Bill Lange. Ich bin Dave Gallo.\n",
      "David Gallo: This is Bill Lange. I'm Dave Gallo.\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 .data/iwslt/de-en/train.de-en.de\n",
    "!head -n 1 .data/iwslt/de-en/train.de-en.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_tpneCe8tvd"
   },
   "source": [
    "Now we build the vocabulary and convert the text corpus into indices. We are going to be replacing tokens that occurred less than 5 times with `<unk>` tokens, and take the rest as our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "oNJM96FA8tvd",
    "outputId": "0924d2de-763e-4f33-a120-4995c82e6a22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common German words: [('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102)]\n",
      "Size of German vocab 13353\n",
      "\n",
      "\n",
      "Most common English words: [('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548)]\n",
      "Size of English vocab 11560\n",
      "\n",
      "\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "\n",
    "print(\"Most common German words:\", DE.vocab.freqs.most_common(5))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Most common English words:\", EN.vocab.freqs.most_common(5))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.45 GiB (GPU 0; 11.17 GiB total capacity; 0 bytes already allocated; 100.56 MiB free; 0 bytes cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3aa66fe18738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading word vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGloVe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"840B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mDE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"de\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, dim, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glove.{}.{}d.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGloVe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munk_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munk_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading vectors from {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_pt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_pt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mroot_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 deserialized_objects[root_key] = restore_location(\n\u001b[0;32m--> 505\u001b[0;31m                     data_type(size), location)\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.45 GiB (GPU 0; 11.17 GiB total capacity; 0 bytes already allocated; 100.56 MiB free; 0 bytes cached)"
     ]
    }
   ],
   "source": [
    "# Loading word vectors\n",
    "EN.vocab.load_vectors(vectors=GloVe(\"840B\"))\n",
    "DE.vocab.load_vectors(vectors=FastText(language=\"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-yGUwxpA8tvg"
   },
   "source": [
    "Now we split our data into batches as usual. Batching for MT is slightly tricky because source/target will be of different lengths. Fortunately, `torchtext` lets you do this by allowing you to pass in a `sort_key` function. This will minimizing the amount of padding on the source side, but since there is still some padding you will inadvertendly \"attend\" to these padding tokens. \n",
    "\n",
    "One way to get rid of padding is to pass a binary `mask` vector to your attention module so its attention score (before the softmax) is minus infinity for the padding token. Another way (which is how we do it for some of our projects) is to manually sort data into batches so that each batch has exactly the same source length (this means that some batches will be less than the desired batch size, though).\n",
    "\n",
    "However, for this homework padding won't matter too much, so it's fine to ignore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yrQ3QmQe8tvq"
   },
   "source": [
    "Let's check to see that the BOS/EOS token is indeed appended to the target (English) sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def batcher(data, batch_size):\n",
    "    # sort first by src len, then by trg len\n",
    "    data = sorted(data, key=lambda x: (len(x.src), len(x.trg)))\n",
    "    curr_batch = []\n",
    "    curr_lengths = None\n",
    "    \n",
    "    # all batches have the same src_len and trg_len\n",
    "    for ex in data:\n",
    "        lengths = (len(ex.src), len(ex.trg))\n",
    "        if (lengths != curr_lengths and curr_batch) or len(curr_batch) == batch_size:\n",
    "            yield curr_batch\n",
    "            curr_batch = []\n",
    "        curr_lengths = lengths\n",
    "        curr_batch.append(ex)\n",
    "        \n",
    "    if curr_batch:\n",
    "        yield curr_batch\n",
    "    \n",
    "class GoodBucketIterator(data.Iterator):\n",
    "    \"\"\"Defines an iterator that batches examples of similar lengths together.\n",
    "    Minimizes amount of padding needed while producing freshly shuffled\n",
    "    batches for each new epoch. See pool for the bucketing procedure used.\n",
    "    \"\"\"\n",
    "    def create_batches(self):\n",
    "        self.batches = list(batcher(self.data(), self.batch_size))\n",
    "        random.shuffle(self.batches)\n",
    "        \n",
    "    def __len__(self):\n",
    "        if hasattr(self, 'batches'):\n",
    "            return len(self.batches)\n",
    "        return super().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda:0')\n",
    "train_iter, val_iter = GoodBucketIterator.splits(\n",
    "    (train, val), batch_size=BATCH_SIZE, device=device, repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2969
    },
    "colab_type": "code",
    "id": "fJse2chy8tvq",
    "outputId": "a4ed1d7c-6a3c-484a-f93f-38bd065e3c1f"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))\n",
    "\n",
    "print(\"Source:\", batch.src.shape)\n",
    "print(\"Target:\", batch.trg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wEg6xGa38tvu"
   },
   "source": [
    "Success! Now that we've processed the data, we are ready to begin modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QEjFvvFg8tvw"
   },
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/harvard-cs287-s19-hw3/\n",
    "\n",
    "For the final Kaggle test, we will provide the source sentence, and you are to predict the **first three words of the target sentence**. The source sentence can be found under `source_test.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "EXcYOO1H8tvx",
    "outputId": "52671600-41df-4d32-aa19-e933cb0fbd02"
   },
   "outputs": [],
   "source": [
    "!curl -Os https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW3/source_test.txt\n",
    "!head -n 2 source_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbMPYOCW8tv1"
   },
   "source": [
    "Similar to HW1, you are to predict the 100 most probable 3-gram that will begin the target sentence. The submission format will be as follows, where each word in the 3-gram will be separated by \"|\", and each 3-gram will be separated by space. For example, here is what an example submission might look like with 5 most-likely 3-grams (instead of 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVSLUg5e8tv2"
   },
   "source": [
    "```\n",
    "Id,Predicted\n",
    "0,Newspapers|talk|about When|I|was Researchers|call|the Twentysomethings|like|Alex But|before|long\n",
    "1,That|'s|what Newspapers|talk|about You|have|robbed It|'s|realizing My|parents|wanted\n",
    "2,We|forget|how We|think|about Proust|actually|links Does|any|other This|is|something\n",
    "3,But|what|do And|it|'s They|'re|on My|name|is It|only|happens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8yp7q_D8tv2"
   },
   "source": [
    "When you print out your data, you will need to escape quotes and commas with the following command so that Kaggle does not complain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lo0EGJRn8tv4"
   },
   "outputs": [],
   "source": [
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-3djaHv8tv6"
   },
   "source": [
    "You should perform your hyperparameter search/early stopping/write-up based on perplexity, not the above metric. In practice, people use a metric called [BLEU](https://www.aclweb.org/anthology/P02-1040.pdf), which is roughly a geometric average of 1-gram, 2-gram, 3-gram, 4-gram precision, with a brevity penalty for producing translations that are too short.\n",
    "\n",
    "The test data associated with `source_test.txt` can be found [here](https://gist.githubusercontent.com/justinchiu/c4340777fa86facd820c59ff4d84c078/raw/e6ec7daba76446bc1000813680f4722060e51900/gistfile1.txt). Compute the BLEU score of your conditional de-en model with the `multi-bleu.perl` script found [here](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl). Please submit your BLEU scores on test with your final writeup using the template provided in the repository:  https://github.com/harvard-ml-courses/nlp-template. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Attentional Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_VECS = EN.vocab.vectors\n",
    "DE_VECS = DE.vocab.vectors\n",
    "\n",
    "EN_embed_size = EN_VECS.shape[1]\n",
    "DE_embed_size = DE_VECS.shape[1]\n",
    "print(EN_embed_size, DE_embed_size)\n",
    "\n",
    "EN_VOCAB_LEN = len(EN.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(ntorch.nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, emb_dropout=0.1, lstm_dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_dropout = ntorch.nn.Dropout(p=emb_dropout)\n",
    "        self.embeddings = ntorch.nn.Embedding.from_pretrained(DE_VECS.clone(), freeze=False)\n",
    "        self.lstm = ntorch.nn.LSTM(DE_embed_size, hidden_size, num_layers, dropout=lstm_dropout) \\\n",
    "                             .spec(\"embedding\", \"srcSeqlen\", \"hidden\")\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.emb_dropout(self.embeddings(x))\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    \n",
    "# TODO: remove duplicated code\n",
    "class DecoderRNN(ntorch.nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, emb_dropout=0.1, lstm_dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_dropout = ntorch.nn.Dropout(p=emb_dropout)\n",
    "        self.embeddings = ntorch.nn.Embedding.from_pretrained(EN_VECS.clone(), freeze=False)\n",
    "        self.lstm = ntorch.nn.LSTM(DE_embed_size, hidden_size, num_layers, dropout=lstm_dropout) \\\n",
    "                             .spec(\"embedding\", \"trgSeqlen\", \"hidden\")\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        emb = self.emb_dropout(self.embeddings(x))\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_IND = EN.vocab.stoi[BOS_WORD]\n",
    "EOS_IND = EN.vocab.stoi[EOS_WORD]\n",
    "\n",
    "def flip(ntensor, dim):\n",
    "    return ntensor\n",
    "    ntensor = ntensor.clone()\n",
    "    idx = ntensor._schema._names.index(dim)\n",
    "    ntensor._tensor = ntensor._tensor.flip(idx)\n",
    "    return ntensor\n",
    "\n",
    "class Seq2Seq(ntorch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.out = ntorch.nn.Linear(decoder.hidden_size, EN_VOCAB_LEN) \\\n",
    "                            .spec(\"hidden\", \"vocab\")\n",
    "    \n",
    "    def _shift_tgt(self, tgt):\n",
    "        start_of_sent = [[BOS_IND] * tgt.shape['batch']]\n",
    "        start_of_sent = ntorch.tensor(start_of_sent, names=('trgSeqlen', 'batch'))\n",
    "        end_of_sent =  tgt[{'trgSeqlen': slice(0, tgt.shape['trgSeqlen'] - 1)}]\n",
    "        shifted = ntorch.cat((start_of_sent, end_of_sent), 'trgSeqlen')\n",
    "        return shifted\n",
    "    \n",
    "    # this function should only be used in training/evaluation\n",
    "    def forward(self, src, tgt):\n",
    "        # TODO: reverse src before encoding\n",
    "        src = flip(src, 'srcSeqlen')\n",
    "        _, enc_hidden = self.encoder(src)\n",
    "        dec_output, _ = self.decoder(self._shift_tgt(tgt), enc_hidden)\n",
    "        out = self.out(dec_output).log_softmax('vocab')\n",
    "        return out\n",
    "    \n",
    "    # this function should implement beam search to translate the src\n",
    "    # src should be (seqLen,) NamedTensor        \n",
    "    def translate(self, src, max_len=30):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # TODO: reverse src before encoding\n",
    "            src = ntorch.tensor(src.values.unsqueeze(0), ('batch', 'srcSeqlen'))\n",
    "            src = flip(src, 'srcSeqlen')\n",
    "            _, enc_hidden = encoder.forward(src)\n",
    "            \n",
    "            dec_input = ntorch.tensor([[EN.vocab.stoi[BOS_WORD]]], ('batch', 'trgSeqlen'))                    \n",
    "            dec_hidden = enc_hidden\n",
    "\n",
    "            translated_sent = []\n",
    "            for i in range(max_len):\n",
    "                dec_output, dec_hidden = decoder.forward(dec_input, dec_hidden)                \n",
    "                prediction = self.out(dec_output).argmax('vocab')\n",
    "                if prediction.item() == EN.vocab.stoi[EOS_WORD]:\n",
    "                    break\n",
    "                translated_sent.append(prediction.item())\n",
    "                dec_input = prediction\n",
    "\n",
    "            return torch.tensor(translated_sent)\n",
    "    \n",
    "    def beam_search(self, src, max_len=30, beam_size=12):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            src = ntorch.tensor(src.values.unsqueeze(0), ('batch', 'srcSeqlen'))\n",
    "            src = flip(src, 'srcSeqlen')\n",
    "            _, enc_hidden = encoder.forward(src)\n",
    "            \n",
    "            translated_sents = []\n",
    "            beams = [(float('-inf'), [BOS_IND])]\n",
    "            \n",
    "            for i in range(max_len):\n",
    "                old_beams = beams[:beam_size]\n",
    "                beams = beams[beam_size:]\n",
    "                for score, beam in old_beams:\n",
    "                    dec_input = ntorch.tensor([beam], ('batch', 'trgSeqlen'))\n",
    "                    dec_output, dec_hidden = decoder.forward(dec_input, enc_hidden)\n",
    "                    last_hidden = dec_hidden[0].get('layers', -1)\n",
    "                    scores = self.out(last_hidden).softmax('vocab').log() / (len(beam) + 1)\n",
    "                    scores, indexes = scores.topk('vocab', beam_size)\n",
    "                    for score, ind in zip(scores.values.flatten(), indexes.values.flatten()):\n",
    "                        beams.append((score.item(), beam + [ind.item()]))\n",
    "                \n",
    "                beams.sort(key=lambda x: x[0], reverse=True)\n",
    "                \n",
    "                new_beams = []\n",
    "                for score, beam in beams:\n",
    "                    if beam[0:2] == [BOS_IND, BOS_IND] and beam[-1] == EOS_IND:\n",
    "                        translated_sents.append((score, beam[1:]))\n",
    "                    else:\n",
    "                        new_beams.append((score, beam))\n",
    "                beams = new_beams\n",
    "                \n",
    "                if len(translated_sents) == beam_size:\n",
    "                    break\n",
    "            \n",
    "            if len(translated_sents) == 0:\n",
    "                translated_sents += beams\n",
    "            \n",
    "            translated_sents.sort(key=lambda x: x[0], reverse=True)\n",
    "            return torch.tensor(translated_sents[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_iter))\n",
    "# src = batch.src[{'batch': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is defined below\n",
    "src, Seq2Seq.beam_search(model, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, batches):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_fn = ntorch.nn.NLLLoss(reduction=\"sum\").spec(\"vocab\")\n",
    "        tot_loss = 0\n",
    "        num_ex = 0\n",
    "        for batch in batches:\n",
    "            log_probs = model.forward(batch.src, batch.trg)\n",
    "            tot_loss += loss_fn(log_probs, batch.trg).values\n",
    "            num_ex += batch.trg.shape['batch'] * batch.trg.shape['trgSeqlen']\n",
    "\n",
    "        # TODO: compute bleu\n",
    "        return torch.exp(tot_loss / num_ex), 0\n",
    "\n",
    "def train_model(model, num_epochs=300, learning_rate=0.001, weight_decay=0, log_freq=1):\n",
    "    model.train()\n",
    "    opt = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "        \n",
    "    loss_fn = ntorch.nn.NLLLoss().spec(\"vocab\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_params = {k: p.detach().clone() for k, p in model.named_parameters()}\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        try:\n",
    "            for batch in tqdm(train_iter, total=len(train_iter)):\n",
    "                opt.zero_grad()\n",
    "\n",
    "                log_probs = model.forward(batch.src, batch.trg)\n",
    "                loss = loss_fn(log_probs, batch.trg)\n",
    "\n",
    "                # compute gradients and update weights\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            # evaluate performance on entire sets\n",
    "            model.eval()\n",
    "            train_loss, train_bleu = evaluate(model, train_iter)\n",
    "            val_loss, val_bleu = evaluate(model, val_iter)\n",
    "            model.train()\n",
    "            \n",
    "            # saving the parameters with the best validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_params = {k: p.detach().clone() for k, p in model.named_parameters()}\n",
    "                best_val_loss = val_loss\n",
    "            \n",
    "            # logging\n",
    "            if i == 0 or i == num_epochs - 1 or (i + 1) % log_freq == 0:\n",
    "                msg = f\"{round(time.time() - start_time)} sec: Epoch {i + 1}\"\n",
    "                print(f'{msg}\\n{\"=\" * len(msg)}')\n",
    "                print(f'Train Perplexity: {train_loss:.5f}\\t Train BLEU: {train_bleu:.2f}%')\n",
    "                print(f'Val Perplexity: {val_loss:.5f}\\t Val BLEU: {val_bleu:.2f}%\\n')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(f'\\nStopped training after {i} epochs...')\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    model.load_state_dict(best_params)\n",
    "                      \n",
    "    msg = f\"{round(time.time() - start_time)} sec: Final Results\"\n",
    "    print(f'{msg}\\n{\"=\" * len(msg)}')\n",
    "\n",
    "    train_loss, train_bleu = evaluate(model, train_iter)\n",
    "    val_loss, val_bleu = evaluate(model, val_iter)\n",
    "    print(f'Train Perplexity: {train_loss:.5f}\\t Train BLEU: {train_bleu:.2f}%')\n",
    "    print(f'Val Perplexity: {val_loss:.5f}\\t Val BLEU: {val_bleu:.2f}%\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(num_layers=3, hidden_size=100, emb_dropout=0.5, lstm_dropout=0.5)\n",
    "decoder = DecoderRNN(num_layers=3, hidden_size=100, emb_dropout=0.5, lstm_dropout=0.5)\n",
    "model = Seq2Seq(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c964ebe79f7240b484ecd9f64d475d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NamedTensor(\n",
      "\ttensor([    2,    24,    31,    26,    26,  8166, 11278,     5,    31,    26,\n",
      "         2155,  3082,     5,    13,   125,    28,    92,   737,     4,     3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([    2,     2,    24,    31,    26,    26,  8166, 11278,     5,    31,\n",
      "           26,  2155,  3082,     5,    13,   125,    28,    92,   737,     4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   14, 1423,   50,  618,   23,    7,  295,   85,   36,   10,   86,\n",
      "          71,    6,  315,   60,   20, 1249,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   14, 1423,   50,  618,   23,    7,  295,   85,   36,   10,\n",
      "          86,   71,    6,  315,   60,   20, 1249,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2, 4754,  942,   18,    0,   45,  870,  765,  199,  169,    5,    6,\n",
      "         229, 2570,   12, 8145,  727,    0,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2, 4754,  942,   18,    0,   45,  870,  765,  199,  169,    5,\n",
      "           6,  229, 2570,   12, 8145,  727,    0,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   48,   11,    6, 1534,  842,    5,    6,    0,  842,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   48,   11,    6, 1534,  842,    5,    6,    0,  842,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   34,   25, 1347,  598, 9885,    5, 2689,   18, 1781,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   34,   25, 1347,  598, 9885,    5, 2689,   18, 1781,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  266,    4,   89,  108,   15,   74,   31,   43, 1736,   21,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  266,    4,   89,  108,   15,   74,   31,   43, 1736,   21]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2, 4305,   63,   15,   36,   26,   46,  500,    9,  293,   38,    8,\n",
      "         422,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2, 4305,   63,   15,   36,   26,   46,  500,    9,  293,   38,\n",
      "           8,  422,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  227,    9,   50, 7382,   17,  147,   23, 6227,    0,    5,   50,\n",
      "        2504,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  227,    9,   50, 7382,   17,  147,   23, 6227,    0,    5,\n",
      "          50, 2504,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   14,   64, 1615,   25, 3774,   35,  562,  252,  317,   29,    6,\n",
      "         551,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   14,   64, 1615,   25, 3774,   35,  562,  252,  317,   29,\n",
      "           6,  551,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   34,   90,  314, 9428,    4,   34,  267, 1040,   98,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   34,   90,  314, 9428,    4,   34,  267, 1040,   98,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   14,  119, 4245,   84,    6,  164,  273,    9,  123,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   14,  119, 4245,   84,    6,  164,  273,    9,  123,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  341,    6, 6515,    0,    4,   27,  488,  121, 8131,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  341,    6, 6515,    0,    4,   27,  488,  121, 8131,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   24,    5,  504,   15,   53,  107,    5, 5508,    4,    0,   39,\n",
      "         112,   15,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   24,    5,  504,   15,   53,  107,    5, 5508,    4,    0,\n",
      "          39,  112,   15,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([    2,    70, 10162,    45,   242,   896,    87,   144,    39,    22,\n",
      "          362,     5,   169,   319,    22,     3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([    2,     2,    70, 10162,    45,   242,   896,    87,   144,    39,\n",
      "           22,   362,     5,   169,   319,    22]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   10,   75,   58, 1539,    5,   10,  166,  935, 1275,    9,  420,\n",
      "          45, 1164,  965,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   10,   75,   58, 1539,    5,   10,  166,  935, 1275,    9,\n",
      "         420,   45, 1164,  965]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   70,   23,    8, 3245,    5,   93,   16,   23,    0,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   70,   23,    8, 3245,    5,   93,   16,   23,    0,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   14, 1097, 7136,  171,    8,  828,    9,    6, 8101,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   14, 1097, 7136,  171,    8,  828,    9,    6, 8101,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2, 250,   0, 646,  32,  12, 120, 236,   7,  15,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2, 250,   0, 646,  32,  12, 120, 236,   7,  15,   4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   34,  166,   16,   17,    8,  271, 7430,  106,    5,    7,   40,\n",
      "        1645,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   34,  166,   16,   17,    8,  271, 7430,  106,    5,    7,\n",
      "          40, 1645,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   76,   85,  111,   16,   26,   13,   21,  129,    5,  133,   78,\n",
      "        1900,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   76,   85,  111,   16,   26,   13,   21,  129,    5,  133,\n",
      "          78, 1900,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  511,   20,  345,    5,  179,   47,    9,   15,   11,    8, 1068,\n",
      "         415,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  511,   20,  345,    5,  179,   47,    9,   15,   11,    8,\n",
      "        1068,  415,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   57, 2507, 1015, 3032,   18, 4086,  989,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   57, 2507, 1015, 3032,   18, 4086,  989,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2, 1165,   63,  155,    9, 4383,    4,  397,  187,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2, 1165,   63,  155,    9, 4383,    4,  397,  187]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  129,    5,   19,  126,   30,   28, 1894,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  129,    5,   19,  126,   30,   28, 1894,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2, 2420,    5,   31,   26,  133,   53, 1024,    4,   34,   43,  739,\n",
      "           4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2, 2420,    5,   31,   26,  133,   53, 1024,    4,   34,   43,\n",
      "         739,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   18,   31,   51,    7, 2390,   71,  526, 3120,    0,    9,    0,\n",
      "           4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   18,   31,   51,    7, 2390,   71,  526, 3120,    0,    9,\n",
      "           0,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   27,   12,   83,    8,  347, 1022, 4150,    0,    0,   17, 2357,\n",
      "           4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   27,   12,   83,    8,  347, 1022, 4150,    0,    0,   17,\n",
      "        2357,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   24,    5,   16,   12,  567,    7,   74,   35,    6,    0,    9,\n",
      "           6,  410,   22, 1015,    4,   22,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   24,    5,   16,   12,  567,    7,   74,   35,    6,    0,\n",
      "           9,    6,  410,   22, 1015,    4,   22]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   24,   19,   36, 2841, 1810,   38,   64, 9933,   18,   19,   36,\n",
      "         195,    6, 1544,    9, 1162,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   24,   19,   36, 2841, 1810,   38,   64, 9933,   18,   19,\n",
      "          36,  195,    6, 1544,    9, 1162,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,  34,  80, 372,  20, 383,  17, 380,  46, 161,   6, 109,   5,  96,\n",
      "         18,  96, 205,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,  34,  80, 372,  20, 383,  17, 380,  46, 161,   6, 109,   5,\n",
      "         96,  18,  96, 205,   4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   97,   12,  524,    9, 4046,   18, 1154, 3961,   18,   46,  500,\n",
      "           9, 2625, 2112,   13,  102,  121,   16,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   97,   12,  524,    9, 4046,   18, 1154, 3961,   18,   46,\n",
      "         500,    9, 2625, 2112,   13,  102,  121,   16,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    0,  475,  377,    5,   49, 1437, 3100,    5,   10, 4227,    6,\n",
      "        4474,   13, 2270,    8,  338,   12,  689,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,    0,  475,  377,    5,   49, 1437, 3100,    5,   10, 4227,\n",
      "           6, 4474,   13, 2270,    8,  338,   12,  689,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   14,  113,   46,    9,   13,  260,   11, 2231,    5,   31,   36,\n",
      "          26,  394,  926,   31,   90,   38,   16,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   14,  113,   46,    9,   13,  260,   11, 2231,    5,   31,\n",
      "          36,   26,  394,  926,   31,   90,   38,   16,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   48,   11,    8, 1916,  597,   13,   19,   80,  847,   67,   17,\n",
      "        3503,    5,   60,   58, 9627, 1916,  597,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   48,   11,    8, 1916,  597,   13,   19,   80,  847,   67,\n",
      "          17, 3503,    5,   60,   58, 9627, 1916,  597,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  192,  108,   15,  111,  225,  137,    5,  137,   87,   40,   92,\n",
      "           9,    6, 1890,   13,  111,  988,  544,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  192,  108,   15,  111,  225,  137,    5,  137,   87,   40,\n",
      "          92,    9,    6, 1890,   13,  111,  988,  544,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   10,  171,   13,  627,    9, 8356,   18,  173,  179, 9440,    5,\n",
      "         173,  179,    0,    5,   50,   53,  224,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   10,  171,   13,  627,    9, 8356,   18,  173,  179, 9440,\n",
      "           5,  173,  179,    0,    5,   50,   53,  224,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NamedTensor(\n",
      "\ttensor([   2, 1424,  176,   23,    6,  887,  176,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2, 1424,  176,   23,    6,  887,  176,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,  22,  89, 177,  15,  37,  40,  21,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,  22,  89, 177,  15,  37,  40,  21]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  190,   12,  184,   35, 1992,  570,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  190,   12,  184,   35, 1992,  570,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([  2, 112,  15,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2, 112,  15,   4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2, 112,  15,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2, 112,  15,   4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2, 112,  15,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2, 112,  15,   4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   27,   12,    6,  166,   63,  784,   29, 1134,  166,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   27,   12,    6,  166,   63,  784,   29, 1134,  166,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   76,    5,   15,   72,    6, 6412, 1838,   73, 1716,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   76,    5,   15,   72,    6, 6412, 1838,   73, 1716,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   14,   61,   10,  816,   16,   23, 8436,    7,  102,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   14,   61,   10,  816,   16,   23, 8436,    7,  102,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   27,   12,   37,   54,   29,    6, 7981,  201,   78,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   27,   12,   37,   54,   29,    6, 7981,  201,   78,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    0,   11,    8,  410,   17, 3617,   13,  292,  289,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,    0,   11,    8,  410,   17, 3617,   13,  292,  289,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   24,   31, 3417,    6, 9569, 3199,  121,  114,  960,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   24,   31, 3417,    6, 9569, 3199,  121,  114,  960,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   27,   12,    8, 1451, 3307,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   27,   12,    8, 1451, 3307,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   34,  140,   38, 1150, 5160,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   34,  140,   38, 1150, 5160,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2, 4315, 1956,   87,   40, 1594,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2, 4315, 1956,   87,   40, 1594,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,    0,   82, 2821,    9, 3066,   18,    6, 6448,  737,   18, 1423,\n",
      "           6, 1134,  737,   11,  390,    0,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,    0,   82, 2821,    9, 3066,   18,    6, 6448,  737,   18,\n",
      "        1423,    6, 1134,  737,   11,  390,    0,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2, 1826,   32,   16,  368,   59,   23, 1132, 1132,    6,  500,    9,\n",
      "         380,   19, 3358,   25,  135,  380,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2, 1826,   32,   16,  368,   59,   23, 1132, 1132,    6,  500,\n",
      "           9,  380,   19, 3358,   25,  135,  380,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  130,   29,   94,    5,    8,   47,   45,   17,   45,    8,   45,\n",
      "         252,  905,   11,  271,  135, 4251,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  130,   29,   94,    5,    8,   47,   45,   17,   45,    8,\n",
      "          45,  252,  905,   11,  271,  135, 4251,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   27,   12, 1109,  260,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   27,   12, 1109,  260,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([ 2,  0, 39,  0,  0,  4,  3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([ 2,  2,  0, 39,  0,  0,  4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2, 1065,   32,   15,   74,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2, 1065,   32,   15,   74,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   97,   12, 1048,    5,   15,  206,  289,    7,  300,   63,  103,\n",
      "          47,  384,   59,   29,    8, 6772,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   97,   12, 1048,    5,   15,  206,  289,    7,  300,   63,\n",
      "         103,   47,  384,   59,   29,    8, 6772,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  128,    0,    5,    6,  168,  512, 4666, 4017, 1956,  805,   35,\n",
      "           6,  151, 5473,    9, 1787,  371,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  128,    0,    5,    6,  168,  512, 4666, 4017, 1956,  805,\n",
      "          35,    6,  151, 5473,    9, 1787,  371,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   10,  384,  217,   32,   81,   84,   38,   16,    5,   18,   81,\n",
      "         101,   81,   54, 8242,   16,   71,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   10,  384,  217,   32,   81,   84,   38,   16,    5,   18,\n",
      "          81,  101,   81,   54, 8242,   16,   71,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   41,   15,  126,  524,    9,   64,  450,   33,    6,  793,    7,\n",
      "        1110,    6, 1180,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   41,   15,  126,  524,    9,   64,  450,   33,    6,  793,\n",
      "           7, 1110,    6, 1180,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  128, 3175,    5,   15,   62,    5,  394,  280,    5,  148,    9,\n",
      "           5,  303,  135,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  128, 3175,    5,   15,   62,    5,  394,  280,    5,  148,\n",
      "           9,    5,  303,  135,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,  14,  20,  11,  32,  15, 364, 113,  15, 364,   8, 472,  56,   8,\n",
      "        499,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,  14,  20,  11,  32,  15, 364, 113,  15, 364,   8, 472,  56,\n",
      "          8, 499,   4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([  2, 903, 434,  62,  32,   8, 987,   0,  11,  21,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2, 903, 434,  62,  32,   8, 987,   0,  11,  21]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([    2,    10,    87,   267,    53,  3786,   697,    64, 10839,     4,\n",
      "            3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([    2,     2,    10,    87,   267,    53,  3786,   697,    64, 10839,\n",
      "            4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,  22, 362,   5,  13, 347,  54, 783,   4,  22,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,  22, 362,   5,  13, 347,  54, 783,   4,  22]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   76,    5,   19,   62,   13,    0,   25,    8,  135, 1164,    9,\n",
      "           0,    5,   18,    0,   11,    0,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   76,    5,   19,   62,   13,    0,   25,    8,  135, 1164,\n",
      "           9,    0,    5,   18,    0,   11,    0,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   14,   10,  207,    5,   22,  362,    5, 9096,    4,   10,  268,\n",
      "          16,    4,   48,   11,  168,    4,   22,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   14,   10,  207,    5,   22,  362,    5, 9096,    4,   10,\n",
      "         268,   16,    4,   48,   11,  168,    4,   22]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   34,   80,  115,    6,  785,   63,   17,  150,  519,   11,    8,\n",
      "        4035,    9,    8,  142,    9, 5105,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   34,   80,  115,    6,  785,   63,   17,  150,  519,   11,\n",
      "           8, 4035,    9,    8,  142,    9, 5105,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2, 9180,   25,  143,  134, 1963,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2, 9180,   25,  143,  134, 1963,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   0, 127, 196, 571, 735,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,   0, 127, 196, 571, 735,   4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   48,   36,   40,   17, 1487,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   48,   36,   40,   17, 1487,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([  2,  52, 680,  30,  26, 313, 318,  38,   6, 779,   9,  82, 178,   4,\n",
      "          3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,  52, 680,  30,  26, 313, 318,  38,   6, 779,   9,  82, 178,\n",
      "          4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   14,   15,   98,  327,    6,  208,   39,  209,   11,    6,  233,\n",
      "        1041,   21,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   14,   15,   98,  327,    6,  208,   39,  209,   11,    6,\n",
      "         233, 1041,   21]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   10,   74,   19,   80,   54, 6119,  421,    8,  432,   45, 4093,\n",
      "         240,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   10,   74,   19,   80,   54, 6119,  421,    8,  432,   45,\n",
      "        4093,  240,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   42,  538,    9, 1265,    5, 2866,  317,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   42,  538,    9, 1265,    5, 2866,  317,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   57,  101,   19,  111,   30,   26, 1924,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   57,  101,   19,  111,   30,   26, 1924,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   27,   12,  152,    6,  343, 1635,  598,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   27,   12,  152,    6,  343, 1635,  598,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   10,   23, 3122,    7,   86,    8,  599, 4678,   49,  294,  117,\n",
      "         242,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   10,   23, 3122,    7,   86,    8,  599, 4678,   49,  294,\n",
      "         117,  242,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  341,   10, 1648,   16,   69,    7, 1030,  517,   13,   23, 2896,\n",
      "        2284,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  341,   10, 1648,   16,   69,    7, 1030,  517,   13,   23,\n",
      "        2896, 2284,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  116,   11,   58,    0,  311,    5,  130,   81,   12,    8,  339,\n",
      "        3762,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  116,   11,   58,    0,  311,    5,  130,   81,   12,    8,\n",
      "         339, 3762,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([  2,  97, 127,  11,   8, 444,   9, 922,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,  97, 127,  11,   8, 444,   9, 922,   4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,  10, 238,  69,  38,  20, 861,  67,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,  10, 238,  69,  38,  20, 861,  67,   4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  227,   23, 1820,   45, 7556, 1635, 3216,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  227,   23, 1820,   45, 7556, 1635, 3216,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,  200,   19,   25,    5,   17,    8,  351,    5,    6, 6517,    5,\n",
      "        2318,  351,    9,    6, 3112,    9,    6,  381,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  200,   19,   25,    5,   17,    8,  351,    5,    6, 6517,\n",
      "           5, 2318,  351,    9,    6, 3112,    9,    6,  381,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   34,  103,  563,  274,   17,    8,  109,   17,  139,  280,   11,\n",
      "          56, 1102,    0,   99, 3263,   60,  258,  119,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   34,  103,  563,  274,   17,    8,  109,   17,  139,  280,\n",
      "          11,   56, 1102,    0,   99, 3263,   60,  258,  119,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   52,   25,  257,   49,    8,  498,  124,   88,    8, 8893, 1026,\n",
      "           9,    6,    0, 3045,   60,  173, 8637,  931,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   52,   25,  257,   49,    8,  498,  124,   88,    8, 8893,\n",
      "        1026,    9,    6,    0, 3045,   60,  173, 8637,  931,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NamedTensor(\n",
      "\ttensor([   2,    0,   38,   47,  949,  420, 1893,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,    0,   38,   47,  949,  420, 1893,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   27,  127, 1855,   33,    6, 2232,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   27,  127, 1855,   33,    6, 2232,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   0,  19, 284,  13,  47, 205,  21,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,   0,  19, 284,  13,  47, 205,  21]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   27,   88,   58,  747,  204, 1383,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   27,   88,   58,  747,  204, 1383,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,  41,  16,  23,  30,  13, 282,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,  41,  16,  23,  30,  13, 282,   4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   27,   12,  567,    7,   40, 6802,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   27,   12,  567,    7,   40, 6802,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   27,  146,  115,    8, 1254,    5,   16,  111,   28, 2008,   56,\n",
      "         314,   69,   56,    8, 1455,    4,   22,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   27,  146,  115,    8, 1254,    5,   16,  111,   28, 2008,\n",
      "          56,  314,   69,   56,    8, 1455,    4,   22]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   14,   61,   10,   80,  120, 4136, 6280,    5,   61,   10,   75,\n",
      "          65,  121, 6280,    5,   78, 6280,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   14,   61,   10,   80,  120, 4136, 6280,    5,   61,   10,\n",
      "          75,   65,  121, 6280,    5,   78, 6280,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   10, 4842,  102,   71,  121,    6, 2974,   18,    6, 7977,   18,\n",
      "          10,  671,   66, 6547,  337,   17,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   10, 4842,  102,   71,  121,    6, 2974,   18,    6, 7977,\n",
      "          18,   10,  671,   66, 6547,  337,   17,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   27,  850,    7, 4909,   71,   29,  137,  308,    5, 3868,  502,\n",
      "           5,   32,   19,  230,    0, 2773,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   27,  850,    7, 4909,   71,   29,  137,  308,    5, 3868,\n",
      "         502,    5,   32,   19,  230,    0, 2773,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   10,   28,  114,  495,    7,  181,   15,   13,   16,   12,   65,\n",
      "           7,    6, 2563,    9,   82, 1611,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   10,   28,  114,  495,    7,  181,   15,   13,   16,   12,\n",
      "          65,    7,    6, 2563,    9,   82, 1611,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   24,   47,    9,   50, 4036, 4287,   71,    5,   15,   86,  278,\n",
      "         495,    9,    0,   96,   46,  709,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   24,   47,    9,   50, 4036, 4287,   71,    5,   15,   86,\n",
      "         278,  495,    9,    0,   96,   46,  709,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   14,    5,   69,   44,    5,   15,   72, 3617,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   14,    5,   69,   44,    5,   15,   72, 3617,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,  14,   6, 330, 800, 199, 222,  53, 682,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,  14,   6, 330, 800, 199, 222,  53, 682,   4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   41,   19,   28,    8,  944, 6503,    9, 2285,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   41,   19,   28,    8,  944, 6503,    9, 2285,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2, 1519,  187,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2, 1519,  187]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2, 733,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2, 733,   4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([2, 0, 4, 3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([2, 2, 0, 4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,  116,   23, 1491,  145, 4141,  317,    8,  709,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  116,   23, 1491,  145, 4141,  317,    8,  709,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   70,   12,    6, 2377,  387,    9,    6, 2443,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   70,   12,    6, 2377,  387,    9,    6, 2443,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  379,    9,   64, 3316, 4191,   25,  271, 2845,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  379,    9,   64, 3316, 4191,   25,  271, 2845,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   76,  333,   36,   40, 9401,   38, 1360,   99,   37,   38, 1360,\n",
      "           4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   76,  333,   36,   40, 9401,   38, 1360,   99,   37,   38,\n",
      "        1360,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2, 2027,  294,  174,    9,    6, 1543, 5371,   55,   86, 3350, 4416,\n",
      "           4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2, 2027,  294,  174,    9,    6, 1543, 5371,   55,   86, 3350,\n",
      "        4416,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  341,   10,  246,    5,  610,    7,  166,    6, 1037, 1113,  848,\n",
      "           4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  341,   10,  246,    5,  610,    7,  166,    6, 1037, 1113,\n",
      "         848,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   27,   12,   51,    5,   19,   25, 1107, 2022,    5,   47,    7,\n",
      "         258,  119,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   27,   12,   51,    5,   19,   25, 1107, 2022,    5,   47,\n",
      "           7,  258,  119,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   27,   12,  148,    9, 8622,  113,   81, 2473,   18,  113,  149,\n",
      "        2473,  215,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   27,   12,  148,    9, 8622,  113,   81, 2473,   18,  113,\n",
      "         149, 2473,  215,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([    2,    52,    80,   115,     7,  1553, 10735,  2839,     4, 10260,\n",
      "            8,  1153,     9,   886,     4,     3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([    2,     2,    52,    80,   115,     7,  1553, 10735,  2839,     4,\n",
      "        10260,     8,  1153,     9,   886,     4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([    2, 10547,     6,   720,  4858,     5,    29,   218,    39,  2129,\n",
      "            4, 10199,   301,     4,     3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([    2,     2, 10547,     6,   720,  4858,     5,    29,   218,    39,\n",
      "         2129,     4, 10199,   301,     4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   97,   25,   30,   53,  151,   78, 1593,   51,   20,   17,    6,\n",
      "         109,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   97,   25,   30,   53,  151,   78, 1593,   51,   20,   17,\n",
      "           6,  109,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   48,   11,  104,  148,    9, 6933,   29,    6, 1118,  161,    6,\n",
      "         619,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   48,   11,  104,  148,    9, 6933,   29,    6, 1118,  161,\n",
      "           6,  619,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([  2, 112,  15,   4, 112,  15,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2, 112,  15,   4, 112,  15,   4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([ 2, 10, 26, 30, 74, 61,  4,  3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([ 2,  2, 10, 26, 30, 74, 61,  4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    0, 4174,   25, 4669,  758,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,    0, 4174,   25, 4669,  758,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([    2,  1424,   176,     5,    33,     6,  1233,     9,     6,  8492,\n",
      "        10549,     5,     6,  3012,  7381,  1648,     4,     3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([    2,     2,  1424,   176,     5,    33,     6,  1233,     9,     6,\n",
      "         8492, 10549,     5,     6,  3012,  7381,  1648,     4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  505,   39,   41,   15,  628,   86,    8,  251,   45,  991, 1755,\n",
      "        1658,   60,   13,  315,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  505,   39,   41,   15,  628,   86,    8,  251,   45,  991,\n",
      "        1755, 1658,   60,   13,  315,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  129,    5,   16,   11,  143,    4,   41,   11,   30,   13, 2963,\n",
      "          17,    6,  164,  106,   21,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  129,    5,   16,   11,  143,    4,   41,   11,   30,   13,\n",
      "        2963,   17,    6,  164,  106,   21]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([  2,  52, 194,  30,  63, 729,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,  52, 194,  30,  63, 729,   4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,  116,  111,   54, 2614,   68,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,  116,  111,   54, 2614,   68,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,  10, 268, 447,  17, 517,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,  10, 268, 447,  17, 517,   4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2, 6238,   19,   84,   58,    0,    9,    6,  132,  978,  475,    9,\n",
      "          66, 4045,   17, 3729,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2, 6238,   19,   84,   58,    0,    9,    6,  132,  978,  475,\n",
      "           9,   66, 4045,   17, 3729,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2, 1495,   51,  294,  174,    5,   56,   10,  101,    5,    9, 4947,\n",
      "        5371,   55,   86,   64,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2, 1495,   51,  294,  174,    5,   56,   10,  101,    5,    9,\n",
      "        4947, 5371,   55,   86,   64,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   14,   20,  231, 7737,   13,   19,   43, 1136,   11,   35,    6,\n",
      "         678,    9,    8,  442,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   14,   20,  231, 7737,   13,   19,   43, 1136,   11,   35,\n",
      "           6,  678,    9,    8,  442,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n",
      "NamedTensor(\n",
      "\ttensor([   2,   70,   12,    8,  135,  607, 1821,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   70,   12,    8,  135,  607, 1821,    4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,  10, 110, 198,  16,   7,  15,   4,   3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,  10, 110, 198,  16,   7,  15,   4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,   14,   20,   11,  202, 3139,   47,    4,    3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([   2,    2,   14,   20,   11,  202, 3139,   47,    4]),\n",
      "\t('trgSeqlen',))\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NamedTensor(\n",
      "\ttensor([  2,  18,  17, 223,   5,  98,  92,  73, 373, 293, 186,   7, 285,   4,\n",
      "          3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,  18,  17, 223,   5,  98,  92,  73, 373, 293, 186,   7, 285,\n",
      "          4]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,  48,  11,  50, 998,   0,   5, 124,  11,  54,   5,  15,  62,   5,\n",
      "          3]),\n",
      "\t('trgSeqlen',))\n",
      "NamedTensor(\n",
      "\ttensor([  2,   2,  48,  11,  50, 998,   0,   5, 124,  11,  54,   5,  15,  62,\n",
      "          5]),\n",
      "\t('trgSeqlen',))\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "start (2) + length (1) exceeds dimension size (2).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-b9e34dd919a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# torch.save(model.state_dict(), \"basic_seq2seq_weights\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-98-552aca9c4954>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, learning_rate, weight_decay, log_freq)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-d25731198895>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'srcSeqlen'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shift_tgt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vocab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-d25731198895>\u001b[0m in \u001b[0;36m_shift_tgt\u001b[0;34m(self, tgt)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshifted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshifted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/namedtensor/torch_helpers.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, name, idx)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         return self._new(\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         )\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: start (2) + length (1) exceeds dimension size (2)."
     ]
    }
   ],
   "source": [
    "train_model(model, num_epochs=100, learning_rate=0.001, weight_decay=0, log_freq=1)\n",
    "# torch.save(model.state_dict(), \"basic_seq2seq_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"basic_seq2seq_weights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_iter = iter(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German: Ich zeige dir damit wo ich Hilfe brauche .\n",
      "English Translated: <s> This 's 's 's 's 's 's 's 's 's 's\n",
      "English Actual: <s> I 'm telling you where I need your help . </s>\n"
     ]
    }
   ],
   "source": [
    "batch = next(train_iter_iter)\n",
    "src = batch.src[{'batch': 0}]\n",
    "trg = batch.trg[{'batch': 0}]\n",
    "\n",
    "translated = model.translate(src, 12)\n",
    "\n",
    "german = ' '.join([DE.vocab.itos[i] for i in src.values])\n",
    "english_translation = ' '.join([EN.vocab.itos[i] for i in translated])\n",
    "english_actual = ' '.join([EN.vocab.itos[i] for i in trg.values])\n",
    "\n",
    "print('German:', german)\n",
    "print('English Translated:', english_translation)\n",
    "print('English Actual:', english_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('srcSeqlen', 15)])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.src.unbind('batch')[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.23"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def bleu(target_file, predictions_file):\n",
    "    cmd = f\"./multi-bleu.perl {target_file} < {predictions_file} \" \\\n",
    "          \"-h 2> /dev/null | cut -d' ' -f3 | cut -d',' -f1\"\n",
    "    return float(subprocess.check_output(cmd, shell=True))\n",
    "    \n",
    "\n",
    "def make_translation_predictions(model, file='test_predictions.txt'):\n",
    "    print('Generating translations')\n",
    "    with open(file, 'w') as outfile:\n",
    "        p = 0\n",
    "        with open('source_test.txt', 'r') as infile:\n",
    "            for line in tqdm(list(infile)):\n",
    "                tokens = [DE.vocab.stoi[w] for w in tokenize_de(line.strip())]\n",
    "                src = ntorch.tensor(tokens, names=\"srcSeqlen\")\n",
    "                translation = model.beam_search(src, 12).tolist()\n",
    "                assert translation[0] == EN.vocab.stoi[BOS_WORD]\n",
    "                sent = ' '.join(EN.vocab.itos[i] for i in translation[1:])\n",
    "                outfile.write(escape(sent) + '\\n')\n",
    "                p += 1\n",
    "                if p < 10:\n",
    "                    print(escape(sent))\n",
    "    \n",
    "def make_kaggle_predictions(model, file='kaggle_predictions.txt'):\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "# make_translation_predictions(model, 'test_predictions.txt')\n",
    "bleu(\"test_predictions.txt\", \"target_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( <unk> ] I was my first slogan was <comma> I was <unk> . </s>\r\n",
      "First <comma> <unk> and 2005 and the early <unk> . </s>\r\n",
      "... ... <unk> <comma> <quote> <unk> <unk> . <quote> </s>\r\n",
      "It was me <comma> but I was a bit of it . </s>\r\n",
      "Another way <comma> and the first <unk> of the first <unk> was a <unk> . </s>\r\n",
      "They wanted to talk about the parents that I was wearing a girl . </s>\r\n",
      "Sorry <comma> I thought <comma> I 'm sorry . </s>\r\n",
      "It did n't know what I did . </s>\r\n",
      "She was <comma> after the first time <comma> she said <comma> <quote> Go back to school <comma> and then I went back to school . </s>\r\n",
      "but I was lucky . I had to see her colleagues . </s>\r\n"
     ]
    }
   ],
   "source": [
    "!head test_predictions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
