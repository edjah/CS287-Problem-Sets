{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kMy8khN8tvG"
   },
   "source": [
    "# HW 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HAY0edmR8tvI"
   },
   "source": [
    "In this homework you will build a full neural machine translation system using an attention-based encoder-decoder network to translate from German to English. The encoder-decoder network with attention forms the backbone of many current text generation systems. See [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) for an excellent tutorial that also contains many modern advances.\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a non-attentional baseline model (pure seq2seq as in [ref](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)). \n",
    "2. Incorporate attention into the baseline model ([ref](https://arxiv.org/abs/1409.0473) but with dot-product attention as in class notes).\n",
    "3. Implement beam search: review/tutorial [here](http://www.phontron.com/slides/nlp-programming-en-13-search.pdf)\n",
    "4. Visualize the attention distribution for a few examples. \n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n",
    "\n",
    "This will be the most time-consuming assignment in terms of difficulty/training time, so we recommend that you get started early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2PbuMxo8tvI"
   },
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. Feel free to construct your models inline, or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "0FNVUhWw85ZW",
    "outputId": "58285e3b-a8e4-4d8e-b037-6881d0c45046"
   },
   "outputs": [],
   "source": [
    "# !pip install -q torch torchtext spacy opt_einsum\n",
    "# !pip install -qU git+https://github.com/harvardnlp/namedtensor\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ea-ruBUP8tvJ"
   },
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# Text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data, datasets\n",
    "\n",
    "# Named Tensor wrappers\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "\n",
    "# Word vectors\n",
    "from torchtext.vocab import GloVe, FastText\n",
    "\n",
    "# utilities for logging time\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AGtH_W2-8tvM"
   },
   "source": [
    "We first need to process the raw data using a tokenizer. We are going to be using spacy, but you can use your own tokenization rules if you prefer (e.g. a simple `split()` in addition to some rules to acccount for punctuation), but we recommend sticking to the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPkWuOiV8tvN"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "teP-tJBp8tvP"
   },
   "source": [
    "Note that we need to add the beginning-of-sentence token `<s>` and the end-of-sentence token `</s>` to the \n",
    "target so we know when to begin/end translating. We do not need to do this on the source side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eq7IABHi8tvQ"
   },
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "\n",
    "DE = NamedField(names=('srcSeqlen',), tokenize=tokenize_de)\n",
    "\n",
    "# only target needs BOS/EOS\n",
    "EN = NamedField(\n",
    "    names=('trgSeqlen',), tokenize=tokenize_en,\n",
    "    init_token = BOS_WORD, eos_token = EOS_WORD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsFvHSZm8tvS"
   },
   "source": [
    "Let's download the data. This may take a few minutes.\n",
    "\n",
    "While this dataset of 200K sentence pairs is relatively small compared to others, it will still take some time to train. We only expect you to work with sentences of length at most 20 for this homework. You are expected to train on at least this reduced dataset for this homework, but are free to experiment with the rest of the training set as well.\n",
    "\n",
    "**We encourage you to start with `MAX_LEN=20` but encourage experimentation after getting reasonable results with the filtered data. The baseline scores are based on models train on the filtered data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "j-s1HsaA8tvT",
    "outputId": "7ae62297-059e-41c2-f322-69ea0aecdb03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': <namedtensor.text.torch_text.NamedField object at 0x7f4b95b5ca20>, 'trg': <namedtensor.text.torch_text.NamedField object at 0x7f4b95b5c9b0>}\n",
      "119076\n",
      "{'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'], 'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.']}\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 20\n",
    "def filter_pred(x):\n",
    "    return len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN\n",
    "    \n",
    "train, val, test = datasets.IWSLT.splits(\n",
    "    exts=('.de', '.en'), fields=(DE, EN), filter_pred=filter_pred,\n",
    ")\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "papOb0pJ8tvY",
    "outputId": "284e365e-9bae-4663-8a7a-8f7a3207f0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Gallo: Das ist Bill Lange. Ich bin Dave Gallo.\n",
      "David Gallo: This is Bill Lange. I'm Dave Gallo.\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 .data/iwslt/de-en/train.de-en.de\n",
    "!head -n 1 .data/iwslt/de-en/train.de-en.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_tpneCe8tvd"
   },
   "source": [
    "Now we build the vocabulary and convert the text corpus into indices. We are going to be replacing tokens that occurred less than 5 times with `<unk>` tokens, and take the rest as our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "oNJM96FA8tvd",
    "outputId": "0924d2de-763e-4f33-a120-4995c82e6a22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common German words: [('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102)]\n",
      "Size of German vocab 13353\n",
      "\n",
      "\n",
      "Most common English words: [('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548)]\n",
      "Size of English vocab 11560\n",
      "\n",
      "\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "\n",
    "print(\"Most common German words:\", DE.vocab.freqs.most_common(5))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Most common English words:\", EN.vocab.freqs.most_common(5))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.54 GiB (GPU 0; 11.17 GiB total capacity; 28.62 MiB already allocated; 533.44 MiB free; 4.97 GiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3aa66fe18738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading word vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGloVe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"840B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mDE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"de\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFastText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munk_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munk_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading vectors from {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_pt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_pt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mroot_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 deserialized_objects[root_key] = restore_location(\n\u001b[0;32m--> 505\u001b[0;31m                     data_type(size), location)\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs287/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.54 GiB (GPU 0; 11.17 GiB total capacity; 28.62 MiB already allocated; 533.44 MiB free; 4.97 GiB cached)"
     ]
    }
   ],
   "source": [
    "# Loading word vectors\n",
    "EN.vocab.load_vectors(vectors=GloVe(\"840B\"))\n",
    "DE.vocab.load_vectors(vectors=FastText(language=\"de\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-yGUwxpA8tvg"
   },
   "source": [
    "Now we split our data into batches as usual. Batching for MT is slightly tricky because source/target will be of different lengths. Fortunately, `torchtext` lets you do this by allowing you to pass in a `sort_key` function. This will minimizing the amount of padding on the source side, but since there is still some padding you will inadvertendly \"attend\" to these padding tokens. \n",
    "\n",
    "One way to get rid of padding is to pass a binary `mask` vector to your attention module so its attention score (before the softmax) is minus infinity for the padding token. Another way (which is how we do it for some of our projects) is to manually sort data into batches so that each batch has exactly the same source length (this means that some batches will be less than the desired batch size, though).\n",
    "\n",
    "However, for this homework padding won't matter too much, so it's fine to ignore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yrQ3QmQe8tvq"
   },
   "source": [
    "Let's check to see that the BOS/EOS token is indeed appended to the target (English) sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def batcher(data, batch_size):\n",
    "    # sort first by src len, then by trg len\n",
    "    data = sorted(data, key=lambda x: (len(x.src), len(x.trg)))\n",
    "    curr_batch = []\n",
    "    curr_lengths = None\n",
    "    \n",
    "    # all batches have the same src_len and trg_len\n",
    "    for ex in data:\n",
    "        lengths = (len(ex.src), len(ex.trg))\n",
    "        if (lengths != curr_lengths and curr_batch) or len(curr_batch) == batch_size:\n",
    "            yield curr_batch\n",
    "            curr_batch = []\n",
    "        curr_lengths = lengths\n",
    "        curr_batch.append(ex)\n",
    "        \n",
    "    if curr_batch:\n",
    "        yield curr_batch\n",
    "    \n",
    "class GoodBucketIterator(data.Iterator):\n",
    "    \"\"\"Defines an iterator that batches examples of similar lengths together.\n",
    "    Minimizes amount of padding needed while producing freshly shuffled\n",
    "    batches for each new epoch. See pool for the bucketing procedure used.\n",
    "    \"\"\"\n",
    "    def create_batches(self):\n",
    "        self.batches = list(batcher(self.data(), self.batch_size))\n",
    "        random.shuffle(self.batches)\n",
    "        \n",
    "    def __len__(self):\n",
    "        if hasattr(self, 'batches'):\n",
    "            return len(self.batches)\n",
    "        return super().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda:0')\n",
    "train_iter, val_iter = GoodBucketIterator.splits(\n",
    "    (train, val), batch_size=BATCH_SIZE, device=device, repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2969
    },
    "colab_type": "code",
    "id": "fJse2chy8tvq",
    "outputId": "a4ed1d7c-6a3c-484a-f93f-38bd065e3c1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: OrderedDict([('srcSeqlen', 1), ('batch', 6)])\n",
      "Target: OrderedDict([('trgSeqlen', 5), ('batch', 6)])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "\n",
    "print(\"Source:\", batch.src.shape)\n",
    "print(\"Target:\", batch.trg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wEg6xGa38tvu"
   },
   "source": [
    "Success! Now that we've processed the data, we are ready to begin modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QEjFvvFg8tvw"
   },
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/harvard-cs287-s19-hw3/\n",
    "\n",
    "For the final Kaggle test, we will provide the source sentence, and you are to predict the **first three words of the target sentence**. The source sentence can be found under `source_test.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "EXcYOO1H8tvx",
    "outputId": "52671600-41df-4d32-aa19-e933cb0fbd02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Als ich in meinen 20ern war , hatte ich meine erste Psychotherapie-Patientin .\r\n",
      "Ich war Doktorandin und studierte Klinische Psychologie in Berkeley .\r\n"
     ]
    }
   ],
   "source": [
    "!curl -Os https://raw.githubusercontent.com/harvard-ml-courses/cs287-s18/master/HW3/source_test.txt\n",
    "!head -n 2 source_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbMPYOCW8tv1"
   },
   "source": [
    "Similar to HW1, you are to predict the 100 most probable 3-gram that will begin the target sentence. The submission format will be as follows, where each word in the 3-gram will be separated by \"|\", and each 3-gram will be separated by space. For example, here is what an example submission might look like with 5 most-likely 3-grams (instead of 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVSLUg5e8tv2"
   },
   "source": [
    "```\n",
    "Id,Predicted\n",
    "0,Newspapers|talk|about When|I|was Researchers|call|the Twentysomethings|like|Alex But|before|long\n",
    "1,That|'s|what Newspapers|talk|about You|have|robbed It|'s|realizing My|parents|wanted\n",
    "2,We|forget|how We|think|about Proust|actually|links Does|any|other This|is|something\n",
    "3,But|what|do And|it|'s They|'re|on My|name|is It|only|happens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8yp7q_D8tv2"
   },
   "source": [
    "When you print out your data, you will need to escape quotes and commas with the following command so that Kaggle does not complain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lo0EGJRn8tv4"
   },
   "outputs": [],
   "source": [
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-3djaHv8tv6"
   },
   "source": [
    "You should perform your hyperparameter search/early stopping/write-up based on perplexity, not the above metric. In practice, people use a metric called [BLEU](https://www.aclweb.org/anthology/P02-1040.pdf), which is roughly a geometric average of 1-gram, 2-gram, 3-gram, 4-gram precision, with a brevity penalty for producing translations that are too short.\n",
    "\n",
    "The test data associated with `source_test.txt` can be found [here](https://gist.githubusercontent.com/justinchiu/c4340777fa86facd820c59ff4d84c078/raw/e6ec7daba76446bc1000813680f4722060e51900/gistfile1.txt). Compute the BLEU score of your conditional de-en model with the `multi-bleu.perl` script found [here](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl). Please submit your BLEU scores on test with your final writeup using the template provided in the repository:  https://github.com/harvard-ml-courses/nlp-template. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Attentional Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 300\n"
     ]
    }
   ],
   "source": [
    "EN_VECS = EN.vocab.vectors\n",
    "DE_VECS = DE.vocab.vectors\n",
    "\n",
    "EN_embed_size = EN_VECS.shape[1]\n",
    "DE_embed_size = DE_VECS.shape[1]\n",
    "print(EN_embed_size, DE_embed_size)\n",
    "\n",
    "EN_VOCAB_LEN = len(EN.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(ntorch.nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, emb_dropout=0.1, lstm_dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_dropout = ntorch.nn.Dropout(p=emb_dropout)\n",
    "        self.embeddings = ntorch.nn.Embedding.from_pretrained(DE_VECS.clone(), freeze=False)\n",
    "        self.lstm = ntorch.nn.LSTM(DE_embed_size, hidden_size, num_layers, dropout=lstm_dropout) \\\n",
    "                             .spec(\"embedding\", \"srcSeqlen\", \"hidden\")\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.emb_dropout(self.embeddings(x))\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    \n",
    "# TODO: remove duplicated code\n",
    "class DecoderRNN(ntorch.nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, emb_dropout=0.1, lstm_dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_dropout = ntorch.nn.Dropout(p=emb_dropout)\n",
    "        self.embeddings = ntorch.nn.Embedding.from_pretrained(EN_VECS.clone(), freeze=False)\n",
    "        self.lstm = ntorch.nn.LSTM(DE_embed_size, hidden_size, num_layers, dropout=lstm_dropout) \\\n",
    "                             .spec(\"embedding\", \"trgSeqlen\", \"hidden\")\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        emb = self.emb_dropout(self.embeddings(x))\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_IND = EN.vocab.stoi[BOS_WORD]\n",
    "EOS_IND = EN.vocab.stoi[EOS_WORD]\n",
    "\n",
    "def flip(ntensor, dim):\n",
    "    return ntensor\n",
    "    ntensor = ntensor.clone()\n",
    "    idx = ntensor._schema._names.index(dim)\n",
    "    ntensor._tensor = ntensor._tensor.flip(idx)\n",
    "    return ntensor\n",
    "\n",
    "class Seq2Seq(ntorch.nn.Module):\n",
    "    def __init__(self, encoder, decoder, teacher_forcing_ratio):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.out = ntorch.nn.Linear(decoder.hidden_size, EN_VOCAB_LEN).spec(\"hidden\", \"vocab\")\n",
    "    \n",
    "    def _shift_trg(self, trg):\n",
    "        start_of_sent = [[BOS_IND] * trg.shape['batch']]\n",
    "        start_of_sent = ntorch.tensor(start_of_sent, names=('trgSeqlen', 'batch'))\n",
    "        end_of_sent =  trg[{'trgSeqlen': slice(0, trg.shape['trgSeqlen'] - 1)}]\n",
    "        shifted = ntorch.cat((start_of_sent, end_of_sent), 'trgSeqlen')\n",
    "        return shifted\n",
    "    \n",
    "    # this function should only be used in training/evaluation\n",
    "    def forward(self, src, trg):\n",
    "        # TODO: reverse src before encoding\n",
    "        src = flip(src, 'srcSeqlen')\n",
    "        _, enc_hidden = self.encoder(src)\n",
    "        \n",
    "        if torch.rand(1) < self.teacher_forcing_ratio:\n",
    "            dec_outputs, _ = self.decoder(self._shift_trg(trg), enc_hidden)\n",
    "            all_log_probs = self.out(dec_outputs).log_softmax('vocab')\n",
    "            return all_log_probs\n",
    "        else:\n",
    "            dec_hidden = enc_hidden\n",
    "            dec_input = ntorch.tensor([[BOS_IND] * trg.shape['batch']], ('trgSeqlen', 'batch'))\n",
    "            all_log_probs = []\n",
    "            for i in range(trg.shape['trgSeqlen']):\n",
    "                dec_output, dec_hidden = self.decoder(dec_input, dec_hidden)\n",
    "                log_probs = self.out(dec_output.get('trgSeqlen', 0)).log_softmax('vocab')\n",
    "                all_log_probs.append(log_probs)\n",
    "                \n",
    "                dec_input = [log_probs.argmax(dim='vocab').values.tolist()]\n",
    "                dec_input = ntorch.tensor(dec_input, ('trgSeqlen', 'batch'))\n",
    "            \n",
    "            return ntorch.stack(all_log_probs, 'trgSeqlen')\n",
    "            \n",
    "            \n",
    "    # this function should implement beam search to translate the src\n",
    "    # src should be (seqLen,) NamedTensor        \n",
    "    def translate(self, src, max_len=30):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # TODO: reverse src before encoding\n",
    "            src = ntorch.tensor(src.values.unsqueeze(0), ('batch', 'srcSeqlen'))\n",
    "            src = flip(src, 'srcSeqlen')\n",
    "            _, enc_hidden = encoder.forward(src)\n",
    "            \n",
    "            dec_input = ntorch.tensor([[EN.vocab.stoi[BOS_WORD]]], ('batch', 'trgSeqlen'))                    \n",
    "            dec_hidden = enc_hidden\n",
    "\n",
    "            translated_sent = []\n",
    "            for i in range(max_len):\n",
    "                dec_output, dec_hidden = decoder.forward(dec_input, dec_hidden)                \n",
    "                prediction = self.out(dec_output).argmax('vocab')\n",
    "                if prediction.item() == EN.vocab.stoi[EOS_WORD]:\n",
    "                    break\n",
    "                translated_sent.append(prediction.item())\n",
    "                dec_input = prediction\n",
    "\n",
    "            return torch.tensor(translated_sent)\n",
    "    \n",
    "    def beam_search(self, src, max_len=30, beam_size=12):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            src = ntorch.tensor(src.values.unsqueeze(0), ('batch', 'srcSeqlen'))\n",
    "            src = flip(src, 'srcSeqlen')\n",
    "            _, enc_hidden = encoder.forward(src)\n",
    "            \n",
    "            translated_sents = []\n",
    "            beams = [(float('-inf'), [BOS_IND])]\n",
    "            \n",
    "            for i in range(max_len):\n",
    "                old_beams = beams[:beam_size]\n",
    "                beams = beams[beam_size:]\n",
    "                for score, beam in old_beams:\n",
    "                    dec_input = ntorch.tensor([beam], ('batch', 'trgSeqlen'))\n",
    "                    dec_output, dec_hidden = decoder.forward(dec_input, enc_hidden)\n",
    "                    last_hidden = dec_hidden[0].get('layers', -1)\n",
    "                    scores = self.out(last_hidden) / (len(beam) + 1)\n",
    "                    scores, indexes = scores.topk('vocab', beam_size)\n",
    "                    for score, ind in zip(scores.values.flatten(), indexes.values.flatten()):\n",
    "                        beams.append((score.item(), beam + [ind.item()]))\n",
    "                \n",
    "                beams.sort(key=lambda x: x[0], reverse=True)\n",
    "                \n",
    "                new_beams = []\n",
    "                for score, beam in beams:\n",
    "                    if beam[0:2] == [BOS_IND, BOS_IND] and beam[-1] == EOS_IND:\n",
    "                        translated_sents.append((score, beam[1:]))\n",
    "                    else:\n",
    "                        new_beams.append((score, beam))\n",
    "                beams = new_beams\n",
    "                \n",
    "                if len(translated_sents) == beam_size:\n",
    "                    break\n",
    "            \n",
    "            if len(translated_sents) == 0:\n",
    "                translated_sents += beams\n",
    "            \n",
    "            translated_sents.sort(key=lambda x: x[0], reverse=True)\n",
    "            return torch.tensor(translated_sents[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(num_layers=3, hidden_size=100, emb_dropout=0.5, lstm_dropout=0.5)\n",
    "decoder = DecoderRNN(num_layers=3, hidden_size=100, emb_dropout=0.5, lstm_dropout=0.5)\n",
    "model = Seq2Seq(encoder, decoder, teacher_forcing_ratio=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, batches):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_fn = ntorch.nn.NLLLoss(reduction=\"sum\").spec(\"vocab\")\n",
    "        tot_loss = 0\n",
    "        num_ex = 0\n",
    "        for batch in batches:\n",
    "            log_probs = model.forward(batch.src, batch.trg)\n",
    "            tot_loss += loss_fn(log_probs, batch.trg).values\n",
    "            num_ex += batch.trg.shape['batch'] * batch.trg.shape['trgSeqlen']\n",
    "\n",
    "        # TODO: compute bleu\n",
    "        return torch.exp(tot_loss / num_ex), 0\n",
    "\n",
    "def train_model(model, num_epochs=300, learning_rate=0.001, weight_decay=0, log_freq=1):\n",
    "    model.train()\n",
    "    opt = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "        \n",
    "    loss_fn = ntorch.nn.NLLLoss().spec(\"vocab\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    best_params = {k: p.detach().clone() for k, p in model.named_parameters()}\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        try:\n",
    "            for batch in tqdm(train_iter, total=len(train_iter)):\n",
    "                opt.zero_grad()\n",
    "\n",
    "                log_probs = model.forward(batch.src, batch.trg)\n",
    "                loss = loss_fn(log_probs, batch.trg)\n",
    "\n",
    "                # compute gradients and update weights\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            # evaluate performance on entire sets\n",
    "            model.eval()\n",
    "            train_loss, train_bleu = evaluate(model, train_iter)\n",
    "            val_loss, val_bleu = evaluate(model, val_iter)\n",
    "            model.train()\n",
    "            \n",
    "            # saving the parameters with the best validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_params = {k: p.detach().clone() for k, p in model.named_parameters()}\n",
    "                best_val_loss = val_loss\n",
    "            \n",
    "            # logging\n",
    "            if i == 0 or i == num_epochs - 1 or (i + 1) % log_freq == 0:\n",
    "                msg = f\"{round(time.time() - start_time)} sec: Epoch {i + 1}\"\n",
    "                print(f'{msg}\\n{\"=\" * len(msg)}')\n",
    "                print(f'Train Perplexity: {train_loss:.5f}\\t Train BLEU: {train_bleu:.2f}%')\n",
    "                print(f'Val Perplexity: {val_loss:.5f}\\t Val BLEU: {val_bleu:.2f}%\\n')\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(f'\\nStopped training after {i} epochs...')\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    model.load_state_dict(best_params)\n",
    "                      \n",
    "    msg = f\"{round(time.time() - start_time)} sec: Final Results\"\n",
    "    print(f'{msg}\\n{\"=\" * len(msg)}')\n",
    "\n",
    "    train_loss, train_bleu = evaluate(model, train_iter)\n",
    "    val_loss, val_bleu = evaluate(model, val_iter)\n",
    "    print(f'Train Perplexity: {train_loss:.5f}\\t Train BLEU: {train_bleu:.2f}%')\n",
    "    print(f'Val Perplexity: {val_loss:.5f}\\t Val BLEU: {val_bleu:.2f}%\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(num_layers=3, hidden_size=100, emb_dropout=0.5, lstm_dropout=0.5)\n",
    "decoder = DecoderRNN(num_layers=3, hidden_size=100, emb_dropout=0.5, lstm_dropout=0.5)\n",
    "model = Seq2Seq(encoder, decoder, teacher_forcing_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd19d7e52194324aeb67dfbc8095b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "103 sec: Epoch 1\n",
      "================\n",
      "Train Perplexity: 54.81414\t Train BLEU: 0.00%\n",
      "Val Perplexity: 55.74067\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df271f9bbf1c433fb21c85226daddf55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "205 sec: Epoch 2\n",
      "================\n",
      "Train Perplexity: 51.02845\t Train BLEU: 0.00%\n",
      "Val Perplexity: 55.43626\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb669703d2cd4f948d8a2f64d05dc972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "310 sec: Epoch 3\n",
      "================\n",
      "Train Perplexity: 49.99533\t Train BLEU: 0.00%\n",
      "Val Perplexity: 55.41478\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c99fc2a41f1434e826d4f16c9fc48a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "414 sec: Epoch 4\n",
      "================\n",
      "Train Perplexity: 46.03027\t Train BLEU: 0.00%\n",
      "Val Perplexity: 50.89476\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1491b9e5524dc7aaf263c4a16c5744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "519 sec: Epoch 5\n",
      "================\n",
      "Train Perplexity: 46.40204\t Train BLEU: 0.00%\n",
      "Val Perplexity: 50.63189\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc8aa75c3da43b380cec50d253855fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "624 sec: Epoch 6\n",
      "================\n",
      "Train Perplexity: 46.84053\t Train BLEU: 0.00%\n",
      "Val Perplexity: 53.37080\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2754abf8044991951f43a129df51ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "728 sec: Epoch 7\n",
      "================\n",
      "Train Perplexity: 43.88587\t Train BLEU: 0.00%\n",
      "Val Perplexity: 45.76720\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640f532da31b4b3cbf7c50f81e8b3098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "833 sec: Epoch 8\n",
      "================\n",
      "Train Perplexity: 42.25760\t Train BLEU: 0.00%\n",
      "Val Perplexity: 51.32739\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f7f6375b0f4d70ae3fbbcb4f499440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "938 sec: Epoch 9\n",
      "================\n",
      "Train Perplexity: 42.15445\t Train BLEU: 0.00%\n",
      "Val Perplexity: 48.50906\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b0298416694875be3c2b4ef72fc53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1043 sec: Epoch 10\n",
      "==================\n",
      "Train Perplexity: 40.15435\t Train BLEU: 0.00%\n",
      "Val Perplexity: 47.89961\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e656a9e0f10d4ad5a8c96b9ab1ee2341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1145 sec: Epoch 11\n",
      "==================\n",
      "Train Perplexity: 37.84677\t Train BLEU: 0.00%\n",
      "Val Perplexity: 46.20785\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86eb2d49e87244fe8eac8e75e3836046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1248 sec: Epoch 12\n",
      "==================\n",
      "Train Perplexity: 37.59453\t Train BLEU: 0.00%\n",
      "Val Perplexity: 41.35464\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ea77c03b084203b98b23c3c3528002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1351 sec: Epoch 13\n",
      "==================\n",
      "Train Perplexity: 38.42088\t Train BLEU: 0.00%\n",
      "Val Perplexity: 48.40092\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d1239745304a09a75da248319f5d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1453 sec: Epoch 14\n",
      "==================\n",
      "Train Perplexity: 37.20375\t Train BLEU: 0.00%\n",
      "Val Perplexity: 44.09346\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548b3fbe514442e3a9a58edd651e78a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1560 sec: Epoch 15\n",
      "==================\n",
      "Train Perplexity: 36.56681\t Train BLEU: 0.00%\n",
      "Val Perplexity: 41.98647\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2626335318754f84bddc4cf0002232a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1664 sec: Epoch 16\n",
      "==================\n",
      "Train Perplexity: 35.34271\t Train BLEU: 0.00%\n",
      "Val Perplexity: 44.26171\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df70ece02e24b04ab6fd07444505fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1770 sec: Epoch 17\n",
      "==================\n",
      "Train Perplexity: 34.91703\t Train BLEU: 0.00%\n",
      "Val Perplexity: 44.59365\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0ba18fbb9f4147843d9388545dc0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1875 sec: Epoch 18\n",
      "==================\n",
      "Train Perplexity: 35.54583\t Train BLEU: 0.00%\n",
      "Val Perplexity: 43.69294\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3192b2536bcc45779314bcc979eacb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1981 sec: Epoch 19\n",
      "==================\n",
      "Train Perplexity: 34.49675\t Train BLEU: 0.00%\n",
      "Val Perplexity: 45.30622\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1544aad202b443c28e4c1409ff237578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2085 sec: Epoch 20\n",
      "==================\n",
      "Train Perplexity: 32.42155\t Train BLEU: 0.00%\n",
      "Val Perplexity: 41.59401\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd1cc2a0400421e90ded1ac47a50fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2185 sec: Epoch 21\n",
      "==================\n",
      "Train Perplexity: 32.96397\t Train BLEU: 0.00%\n",
      "Val Perplexity: 35.95562\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147cd8771f76489286ab4288a068c75d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2287 sec: Epoch 22\n",
      "==================\n",
      "Train Perplexity: 32.01445\t Train BLEU: 0.00%\n",
      "Val Perplexity: 43.18745\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bf8e84c3a74090a696367fdafa6381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2388 sec: Epoch 23\n",
      "==================\n",
      "Train Perplexity: 31.63923\t Train BLEU: 0.00%\n",
      "Val Perplexity: 43.44305\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b10122fd8de491d8515eaa22cb3e010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2487 sec: Epoch 24\n",
      "==================\n",
      "Train Perplexity: 32.37148\t Train BLEU: 0.00%\n",
      "Val Perplexity: 44.35393\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ee3724963f46f0a015c47541124fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2591 sec: Epoch 25\n",
      "==================\n",
      "Train Perplexity: 30.84382\t Train BLEU: 0.00%\n",
      "Val Perplexity: 44.93626\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ece237dbeb4c6bbc9624afcf40a6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2697 sec: Epoch 26\n",
      "==================\n",
      "Train Perplexity: 31.49244\t Train BLEU: 0.00%\n",
      "Val Perplexity: 41.28381\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51856e5a16543cca06afda51ef03d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2798 sec: Epoch 27\n",
      "==================\n",
      "Train Perplexity: 31.09934\t Train BLEU: 0.00%\n",
      "Val Perplexity: 42.03200\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2daadb7710462f943668440fec1bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2901 sec: Epoch 28\n",
      "==================\n",
      "Train Perplexity: 29.26152\t Train BLEU: 0.00%\n",
      "Val Perplexity: 40.14204\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4730821f59fd47679fa6a929fe0a7124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3004 sec: Epoch 29\n",
      "==================\n",
      "Train Perplexity: 29.76007\t Train BLEU: 0.00%\n",
      "Val Perplexity: 40.01583\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb75fbd5cf9f418c95d9e0aad343cc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3108 sec: Epoch 30\n",
      "==================\n",
      "Train Perplexity: 29.06407\t Train BLEU: 0.00%\n",
      "Val Perplexity: 40.58934\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dd7c6049da4d038f5d01d74ee32f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3210 sec: Epoch 31\n",
      "==================\n",
      "Train Perplexity: 28.20963\t Train BLEU: 0.00%\n",
      "Val Perplexity: 35.80866\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d51cf3d1935442da27d7eca6a4a9998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3313 sec: Epoch 32\n",
      "==================\n",
      "Train Perplexity: 28.27464\t Train BLEU: 0.00%\n",
      "Val Perplexity: 41.68919\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974b2e8977224ca49b5f81c5c37e0be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3416 sec: Epoch 33\n",
      "==================\n",
      "Train Perplexity: 27.96127\t Train BLEU: 0.00%\n",
      "Val Perplexity: 41.57233\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8627ec13ee4a83b03f37cd49da2ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3515 sec: Epoch 34\n",
      "==================\n",
      "Train Perplexity: 27.62259\t Train BLEU: 0.00%\n",
      "Val Perplexity: 38.07216\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a65f9d015d74d8c8c1505af699212aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3621 sec: Epoch 35\n",
      "==================\n",
      "Train Perplexity: 28.42461\t Train BLEU: 0.00%\n",
      "Val Perplexity: 40.10124\t Val BLEU: 0.00%\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8b019c2f1a46748f3d4a71d65e8f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stopped training after 35 epochs...\n",
      "3712 sec: Final Results\n",
      "=======================\n",
      "Train Perplexity: 28.96623\t Train BLEU: 0.00%\n",
      "Val Perplexity: 40.79209\t Val BLEU: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, num_epochs=100, learning_rate=0.001, weight_decay=0, log_freq=1)\n",
    "# torch.save(model.state_dict(), \"basic_seq2seq_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"basic_seq2seq_weights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_iter = iter(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German: Ich selbst machte eine unerwartete Erfahrung .\n",
      "English Translated: <s> I I a a , . . .\n",
      "English Actual: <s> Here 's some personal input I did n't expect . </s>\n"
     ]
    }
   ],
   "source": [
    "batch = next(train_iter_iter)\n",
    "src = batch.src[{'batch': 0}]\n",
    "trg = batch.trg[{'batch': 0}]\n",
    "\n",
    "translated = model.translate(src, 12)\n",
    "\n",
    "german = ' '.join([DE.vocab.itos[i] for i in src.values])\n",
    "english_translation = ' '.join([EN.vocab.itos[i] for i in translated])\n",
    "english_actual = ' '.join([EN.vocab.itos[i] for i in trg.values])\n",
    "\n",
    "print('German:', german)\n",
    "print('English Translated:', english_translation)\n",
    "print('English Actual:', english_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('srcSeqlen', 7)])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.src.unbind('batch')[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating translations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31a016155ad49209cb2a7f2ec4fe831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=800), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As As\n",
      "and I was\n",
      "Buckminster It\n",
      "As I\n",
      "As I\n",
      "As I\n",
      "But I\n",
      "Like I\n",
      "So and\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def bleu(target_file, predictions_file):\n",
    "    cmd = f\"./multi-bleu.perl {target_file} < {predictions_file} \" \\\n",
    "          \"-h 2> /dev/null | cut -d' ' -f3 | cut -d',' -f1\"\n",
    "    return float(subprocess.check_output(cmd, shell=True))\n",
    "    \n",
    "\n",
    "def make_translation_predictions(model, file='test_predictions.txt'):\n",
    "    print('Generating translations')\n",
    "    with open(file, 'w') as outfile:\n",
    "        p = 0\n",
    "        with open('source_test.txt', 'r') as infile:\n",
    "            for line in tqdm(list(infile)):\n",
    "                tokens = [DE.vocab.stoi[w] for w in tokenize_de(line.strip())]\n",
    "                src = ntorch.tensor(tokens, names=\"srcSeqlen\")\n",
    "                translation = model.beam_search(src, 3).tolist()\n",
    "                assert translation[0] == EN.vocab.stoi[BOS_WORD]\n",
    "                sent = ' '.join(EN.vocab.itos[i] for i in translation[1:])\n",
    "                outfile.write(escape(sent) + '\\n')\n",
    "                p += 1\n",
    "                if p < 10:\n",
    "                    print(escape(sent))\n",
    "    \n",
    "def make_kaggle_predictions(model, file='kaggle_predictions.txt'):\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "make_translation_predictions(model, 'test_predictions.txt')\n",
    "bleu(\"test_predictions.txt\", \"target_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( <unk> ] I was my first slogan was <comma> I was <unk> . </s>\r\n",
      "First <comma> <unk> and 2005 and the early <unk> . </s>\r\n",
      "... ... <unk> <comma> <quote> <unk> <unk> . <quote> </s>\r\n",
      "It was me <comma> but I was a bit of it . </s>\r\n",
      "Another way <comma> and the first <unk> of the first <unk> was a <unk> . </s>\r\n",
      "They wanted to talk about the parents that I was wearing a girl . </s>\r\n",
      "Sorry <comma> I thought <comma> I 'm sorry . </s>\r\n",
      "It did n't know what I did . </s>\r\n",
      "She was <comma> after the first time <comma> she said <comma> <quote> Go back to school <comma> and then I went back to school . </s>\r\n",
      "but I was lucky . I had to see her colleagues . </s>\r\n"
     ]
    }
   ],
   "source": [
    "!head test_predictions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
